{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47eeba14-d3a6-41a9-82c3-5b097534a7e6",
   "metadata": {},
   "source": [
    "# Name: Mohammad Javad Noroozi\n",
    "# Student Number: 99102434\n",
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19a1c3",
   "metadata": {},
   "source": [
    "### import the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2206d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 12:23:47 WARN Utils: Your hostname, javad-IdeaPad-Gaming-3-15IAH7 resolves to a loopback address: 127.0.1.1; using 172.20.24.150 instead (on interface wlp0s20f3)\n",
      "24/02/06 12:23:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/06 12:23:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/06 12:23:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/02/06 12:23:48 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/02/06 12:23:48 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/02/06 12:23:48 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming Process Files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd440c98f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "import findspark\n",
    "findspark.find()\n",
    "findspark.init()\n",
    "import re\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col\n",
    "from pyspark.sql.types import StructType, StringType, StructField, ArrayType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import findspark\n",
    "\n",
    "\n",
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming Process Files\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f496e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "necessary_keys = [\n",
    "    'text',\n",
    "    ]\n",
    "my_dict1 = {}\n",
    "my_dict2 = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa94c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hash1(x):\n",
    "    s = x\n",
    "    return int(hashlib.sha256(s.encode('utf-8')).hexdigest(), 16) % 103\n",
    "    a = 13\n",
    "    b = 7\n",
    "    p = 103\n",
    "    return (a * x[0] + b) % p\n",
    "\n",
    "\n",
    "def my_hash2(x):\n",
    "    a = 23\n",
    "    b = 71\n",
    "    p = 103\n",
    "    return (a * x + b) % p\n",
    "\n",
    "# def update_dict(word, count, my_dict, my_hash):\n",
    "#     # my_dict = {}\n",
    "#     # word, count = word_count\n",
    "#     key = my_hash(word)\n",
    "#     if my_dict.get(key):\n",
    "#         if my_dict[key][0] == word:\n",
    "#             my_dict[key][1] += count\n",
    "        \n",
    "#         elif my_dict[key][1] <= count + 1:\n",
    "#             my_dict[key] = (word, count)\n",
    "#         else:\n",
    "#             my_dict[key][1] -= count\n",
    "#     else:\n",
    "#         my_dict[key] = (word, count)\n",
    "\n",
    "# def update_dict(word, count):\n",
    "#     # my_dict = {}\n",
    "#     # word, count = word_count\n",
    "#     key = update_dict.my_hash(word)\n",
    "#     if update_dict.my_dict.get(key):\n",
    "#         if update_dict.my_dict[key][0] == word:\n",
    "#             # print(update_dict.my_dict[key][1], count)\n",
    "#             update_dict.my_dict[key][1] += count\n",
    "        \n",
    "#         elif update_dict.my_dict[key][1] <= count + 1:\n",
    "#             update_dict.my_dict[key] = [word, count]\n",
    "#         else:\n",
    "#             update_dict.my_dict[key][1] -= count\n",
    "#     else:\n",
    "#         update_dict.my_dict[key] = [word, count]\n",
    "\n",
    "\n",
    "def update_dict(word, count):\n",
    "    global my_dict\n",
    "    # word, count = word_count\n",
    "    key = my_hash1(word)\n",
    "    if my_dict.get(key):\n",
    "        if my_dict[key][0] == word:\n",
    "            # print(update_dict.my_dict[key][1], count)\n",
    "            my_dict[key][1] += count\n",
    "        \n",
    "        elif my_dict[key][1] <= count + 1:\n",
    "            my_dict[key] = [word, count]\n",
    "        else:\n",
    "            my_dict[key][1] -= count\n",
    "    else:\n",
    "        my_dict[key] = [word, count]\n",
    "    \n",
    "\n",
    "\n",
    "def update_dicts(x):\n",
    "    global my_dict\n",
    "    # print('----------------------',x)\n",
    "    update_dict(x[0], x[1])\n",
    "    # if my_hash1(x[0]) % 10 == 0:\n",
    "    #     print(update_dict.my_dict)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdf5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_hash(x):\n",
    "#     a = 13\n",
    "#     b = 7\n",
    "#     p = 103\n",
    "#     return (a * x + b) % p\n",
    "\n",
    "# def update_dict(word):\n",
    "#     count = 1\n",
    "#     key = my_hash(word)\n",
    "#     if update_dict.my_dict.get(key):\n",
    "#         if update_dict.my_dict[key][0] == word:\n",
    "#             update_dict.my_dict[key][1] += count\n",
    "        \n",
    "#         elif update_dict.my_dict[key][1] <= count + 1:\n",
    "#             update_dict.my_dict[key] = (word, count)\n",
    "#         else:\n",
    "#             update_dict.my_dict[key][1] -= count\n",
    "#     else:\n",
    "#         update_dict.my_dict[key] = (word, count)\n",
    "\n",
    "\n",
    "# update_dict.my_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fa0e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 12:23:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/02/06 12:23:50 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d2f50227-d354-4141-bdb7-19e8f332327d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/02/06 12:23:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "ERROR:root:KeyboardInterrupt while sending command.                             \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m filtered_exploded_df\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     39\u001b[0m query \u001b[38;5;241m=\u001b[39m word_counts\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mforeach(update_dicts)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# # word_counts.foreach(update_dicts)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# # sorted_word_counts = word_counts.orderBy(F.desc(\"count\"))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#     .format(\"console\") \\\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#     .start()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global my_dict\n",
    "my_dict = {}\n",
    "\n",
    "file_path = \"/home/javad/venvspark/notebooks/Project/databricks-datasets/structured-streaming/events\"\n",
    "# file_path = \"/home/javad/venvspark/notebooks/HW1and2/data\"\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"TweetsWordCount\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "# Read the JSON file as a stream\n",
    "streaming_df = spark.readStream.schema(schema).json(file_path)\n",
    "\n",
    "\n",
    "# Explode the list of tweet texts\n",
    "# exploded_df = streaming_df.select(explode(col(\"text\")).alias(\"text\"))\n",
    "\n",
    "cleaned_df = streaming_df.withColumn(\"cleaned_text\", regexp_replace(col(\"text\"), \"[^آ-ی\\s\\\\n]+\", \"\"))\n",
    "\n",
    "# Split the tweet text into words\n",
    "words_df = cleaned_df.select(explode(split(col(\"cleaned_text\"), \" \")).alias(\"word\"))\n",
    "# words_df = streaming_df.select(explode(re.sub('[^\\w\\s]+', '', col(\"text\"))).alias(\"word\"))\n",
    "\n",
    "\n",
    "# Iterate over the words and update the dictionary\n",
    "# query = words_df.writeStream.foreach(update_dicts).start()\n",
    "# Start the streaming query\n",
    "# query.awaitTermination()\n",
    "\n",
    "stop_words = set(\" گردد نحوه  دوباره‌ای خواهیمكرد توانند می‌توانند می فرماید می‌فرماید وجود دارد باشیم بنابراین برخوردار نمیمونید ساخته‌ام میخواستن خواستن خواستند خواستیم نمیگیرند همچنانكه نمیمونید میخواهند تواند باشند می‌تواند می‌باشند استفاده  جلوگیری نمیكنند رابسیار مینوشند نمیاورم رفته رفته‌ام اینطوری شده‌اند می شوند می کنند می توان می باشد می‌شوند می‌کنند می‌توان می‌باشد کند هدف کلی چنین همچنین  درباره بسیاری اساس براساس خواهند توانند اولین دومین آخرین نهایی نخستین تاكنون برداری بهترین بیشتری داشتند نخواهد دیگران همچنان ندارند روز روزهای یكدیگر همواره گذاشته  نداشته خواهیم نخواهیم درون بیرونِ پایین پاعینِ نزدیكِ دنبالِ دنبال برابر برابرِ مانند مانندِ هنگامِ هنگام عنوانِ عنوان البتّه البته وقتیكه هنگامی میكنند همانند پیششون میباشد میگردد درباره میدهیم بینمان مهمتر مهمتره بیاریم نمیبرد نمیروند نمیروم شناسید بریزید میكنیم علارغم نمیكنن اینجور میگیره میكنند میشوند می شود می کند می‌شود می‌کند می آید می‌آید این ها این جا اینجا اینها این‌ها نخواهد نخواهند خواهند خواهد هستند داشته کردیم نکردیم نکرد نکردند كردند دارند بیشتر بسیار گرفته تواند اینكه مختلف گزاری گذاری بودند دیروز امروز باشند ندارد دیگری اكنون البته مقابل امسال  قرار پارسال مانند جریان ساخته نزدیك دانستند ندانست دانست تمامی ایشان ندادند دادند هنگام نباید نداریم دارند ندارند داریم چگونه خواست هستند هست هستید هستیم سراسر گروهی آورده نماید باشی باشم باشند باشیم گویند شناسی نداشتن داشتن همچون نكرده دانند بالای خارجِ كنارِ کنار حدود حدودِ علت علّتِ كجاست چندین لطفا لطفاً مدتی مدت مدّتی میكنن هممون زدند نزدند زد نزد زدیم نزدیم دارای نشود نشوند بشود بشوند یگردی عمدا عمدتا آورده برای وبرای بتوان نتوان نتوانیم نتوانند ندیدی خوبی بخوبی بدهید نکنیم نكنند بوجود میكنم میکنیم میکنید میكند میكنه دانست دانستن دانستند دونید گرفتن گرفتند گرفتید بگیرد بگیرند بگیرم زدن زدم زدی زدیم زدند بزنیم بزن بزنند بزنید توانستید توانستن توانست تونست نداشتیم نداشتم نداشتند نداشتید نداشتی نداشت ندارم نداریم ندارید نداری نداره کردیم کردند کردن کردم كردید بود بودن بودند نبود نبودید نبودن نبودند بودید واقعا هستند هستیم هست هستید اینجا بهشون ایشان میكنن میکنند میکنیم میکنید میکنم میکند میکرد کرد کردم میکردم میرم میروم اونجا میدهم میداد میدهد كردیم می شد می‌شد میشد  میان حالی درحال درحالی سر سایر خصوص میتواند کل جز میشود کرده  کامل برخی غیر شوند میدهد بی دادند پی پس افزود وجود پیش بین وجود گرفته داشته ضمن خوب دهند رسیده میشوند رسید چون چرا برخی بودن من تو بدون طی ها نه کننده چند حتی گرفت بودند باشند هستند هستید هست هستیم چه بوده دارند مگر فقط کسی همانطور همچنان میکند نمیشود میشد اوست نگه هم آنچنان باشیم باشید یکی روی اینطور باشم باشد ادامه میخوانید میکنم کنید کنیم کنند کند کرد کردم کردن کردند داد هرچه همه هر اما های خواهد داد دیگر نیز می بود هم بر تا شد  خود حال اظهار بیان اشاره خودش خودم خودمان هست داشت کرد شما آنها این آن را با تا برای و در به از که  نسبت ترتیب  باهم نمیآید شماست آنهاست ماست برایمان برایشان برایتان بتوانیم نتوانیم بتوان نتوان اینکه  یا او آن ها آنها آنجا آن جا آن‌ها NULL برای كنند دارد كرده باید مورد آنها دیگر مردم صورت نیست است و از با در باید وی او ما شما آنها من بنده داشت داده بوده همین شوند هایی جدید كردن كنیم نشان توسط چنین برخی گیرد گفته آنان گرفت دهند بدون تمام شدند ترین قابل گوید همان آمده طریق گیری هنوز كنید سازی بلكه بودن وقتی یابد تازه آورد آنچه نخست نشده شاید زیرا ریزی گردد یک كمتر دادن ناشی چیزی آنكه بالا بعضی نبود دیده بندی نوعی آنجا كردم  رفته زیاد خویش نظیر باره جایی زیرِ زیر جلوی پیشِ پیش عقب عقبِ وسطِ وسط نزدِ نزد طبقِ طبق ضد ضدِّ مثلِ مثل بارة اثرِ اثر تول تولِ سمتِ سمت قصد قصدِ چیست چطور كدام بعری هرگز تنها خیاه مرسی كنند خودی وگرد بازم رویم ابدا میگن پراز هشان نریم واسش خاست هایت بشید دچار داره بعدا گرچه میرم باشه شدید نشون رسید دارم بیاد بودم هستن میشه برید بریم وارد یعنی اینا بدان نباش نداد میرن شدیم خودم اونا خیلی نكنه باشم کردن 0000 این است های شود شده خود كرد كند بود گفت نیز اما اند نمی بین پیش اگر همه یکی دهد داد راه سوی روی اول نام هیچ چند بیش شدن حتی ولی دوم بعد شما بار طور چون قبل تحت جای چرا سوم كنم زیر ضمن فقط آید فكر پنج كسی رفت مثل آمد ایم غیر وگو فوق سعی سپس شان هفت سری توی روب جدا هست كجا كَی آیا مگر چیز بله بلی آره آری مان تان كنن ونن مند آخه ماا بشم كنه كنی تون بهم هاش بشی ازش روى بشن بری الی كلی جور اون چیه توش بدم مون  كنن 000 های در به از که کن می را با آن یک ها شد ای تا بر وی هم ما یا سه دو هر او پس بی من چه نه طی كل تر حق اش ام ده شش جز كی یك یک جا كس ور تو را چی گه یه شو رو اه ات  چهار نا 00 ره عج ی و ع ص ۱ ۲ ۳ ۴ ۵ ۶ ۷ ۸ ۹ ۰ a b c d e t o p x y z\".split())\n",
    "\n",
    "filtered_exploded_df = words_df.filter(~F.col(\"word\").isin(list(stop_words)))\n",
    "\n",
    "# # Count the occurrences of each word\n",
    "word_counts = filtered_exploded_df.groupBy(\"word\").count()\n",
    "\n",
    "query = word_counts.writeStream.outputMode(\"update\").foreach(update_dicts).start()\n",
    "query.awaitTermination()\n",
    "\n",
    "# # word_counts.foreach(update_dicts)\n",
    "\n",
    "# # sorted_word_counts = word_counts.orderBy(F.desc(\"count\"))\n",
    "\n",
    "# query = word_counts \\\n",
    "#     .writeStream \\\n",
    "#     .foreach(update_dicts)\\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46466995",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956eedbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global my_dict\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af7dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global a\n",
    "a = {}\n",
    "\n",
    "def f():\n",
    "    global a\n",
    "    a[1] = 1\n",
    "\n",
    "def g():\n",
    "    f()\n",
    "    f()\n",
    "print(a)\n",
    "g()\n",
    "a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
